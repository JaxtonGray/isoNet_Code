{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing and Combination\n",
    "------------------------------\n",
    "\n",
    "1. Extract the isotope data from the original CNIP data\n",
    "2. Remove unnecessary columns and clean the data\n",
    "3. Extract the needed information from the HydroGFD data\n",
    "4. Combine CNIP and HydroGFD data into one dataframe\n",
    "5. Remove rows with missing values and save the data\n",
    "\n",
    "I will begin by importing the libraries I will need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract the isotope data from the original CNIP data\n",
    "\n",
    "The original dataset is has each station separated by sheet which is not helpful. So In this section we will be taking that data in and extracting the isotope data from it. From here I will be combining the data from the different sheets into one dataframe. The columns that I will be extracting are the following:\n",
    "* Station\n",
    "* Lat\n",
    "* Lon\n",
    "* Alt\n",
    "* Date\n",
    "* Month\n",
    "* O18avg\n",
    "* H2avg\n",
    "* Dex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>O18(1)</th>\n",
       "      <th>O18(2)</th>\n",
       "      <th>O18Avg</th>\n",
       "      <th>H2(1)</th>\n",
       "      <th>H2(2)</th>\n",
       "      <th>H2avg</th>\n",
       "      <th>dex</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Prec(1)</th>\n",
       "      <th>Prec(2)</th>\n",
       "      <th>Prec(3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-02-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-50.700</td>\n",
       "      <td>33.460</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>84.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-03-02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-48.040</td>\n",
       "      <td>20.800</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>164.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-04-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-10.880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.485</td>\n",
       "      <td>59.555</td>\n",
       "      <td>1.7</td>\n",
       "      <td>95.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-05-02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-50.455</td>\n",
       "      <td>9.945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-06-02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-76.480</td>\n",
       "      <td>-29.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-03-02</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3266</th>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-04-02</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-05-02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-07-02</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3270 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Station    Lat   Long    Alt       Date  Month O18(1) O18(2)  O18Avg  \\\n",
       "0        BAB  47.98  55.82  190.0 1997-02-02    2.0    NaN    NaN -10.520   \n",
       "1        BAB  47.98  55.82  190.0 1997-03-02    3.0    NaN    NaN  -8.605   \n",
       "2        BAB  47.98  55.82  190.0 1997-04-02    4.0    NaN    NaN -10.880   \n",
       "3        BAB  47.98  55.82  190.0 1997-05-02    5.0    NaN    NaN  -7.550   \n",
       "4        BAB  47.98  55.82  190.0 1997-06-02    6.0    NaN    NaN  -5.835   \n",
       "...      ...    ...    ...    ...        ...    ...    ...    ...     ...   \n",
       "3265     HAL  68.47  81.15    8.0 2007-03-02    3.0    NaN    NaN     NaN   \n",
       "3266     HAL  68.47  81.15    8.0 2007-04-02    4.0    NaN    NaN     NaN   \n",
       "3267     HAL  68.47  81.15    8.0 2007-05-02    5.0    NaN    NaN     NaN   \n",
       "3268     HAL  68.47  81.15    8.0 2007-06-02    6.0    NaN    NaN     NaN   \n",
       "3269     HAL  68.47  81.15    8.0 2007-07-02    7.0    NaN    NaN     NaN   \n",
       "\n",
       "     H2(1)  H2(2)   H2avg     dex  Temp  Prec(1)  Prec(2) Prec(3)  \n",
       "0      NaN    NaN -50.700  33.460  -8.2     84.5      NaN     8.1  \n",
       "1      NaN    NaN -48.040  20.800  -6.4    164.6      NaN    12.1  \n",
       "2      NaN    NaN -27.485  59.555   1.7     95.4      NaN     9.0  \n",
       "3      NaN    NaN -50.455   9.945   NaN      NaN      NaN    12.4  \n",
       "4      NaN    NaN -76.480 -29.800   NaN      NaN      NaN     9.2  \n",
       "...    ...    ...     ...     ...   ...      ...      ...     ...  \n",
       "3265   NaN    NaN     NaN     NaN   NaN      NaN      NaN     NaN  \n",
       "3266   NaN    NaN     NaN     NaN   NaN      NaN      NaN     NaN  \n",
       "3267   NaN    NaN     NaN     NaN   NaN      NaN      NaN     NaN  \n",
       "3268   NaN    NaN     NaN     NaN   NaN      NaN      NaN     NaN  \n",
       "3269   NaN    NaN     NaN     NaN   NaN      NaN      NaN     NaN  \n",
       "\n",
       "[3270 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the stations within the CNIP dataset to extract\n",
    "stationList = [\"BAB\", \"BON\", \"CPA\", \"EGB\", \"ELA\", \"EST\", \"GOB\", \"HAB\", \"SAT\", \"SNA\", \"SKT\", \"OTT\", \"BRA\", \"CAM\", \"EUR\", \"RES\", \"ALR\", \"HAL\"]\n",
    "cnipDict = {} # Dictionary to store the CNIP data\n",
    "stationCoords = {} # Dict to store the station coordinates\n",
    "# Loop through each station and extract the data\n",
    "for station in stationList:\n",
    "    # Read in the data\n",
    "    data = pd.read_excel(\"CNIP Updated Data Stations 10.08.2009..xls\", header=None, sheet_name=station, skiprows=[0, 1])\n",
    "\n",
    "    #Making sure the columns are filled properly with constant values: Station, Lat, Long, Alt\n",
    "    data[0] = station #Station\n",
    "    data[1] = data[1].iloc[0] #Lat\n",
    "    data[2] = data[2].iloc[0] #Long\n",
    "    data[3] = data[3].iloc[0] #Alt\n",
    "    stationCoords[station] = tuple([data[1].iloc[0], data[2].iloc[0]]) #Store the station coordinates\n",
    "    #Add the station data to the dictionary\n",
    "    cnipDict[station] = data\n",
    "\n",
    "columns = [\"Station\", \"Lat\", \"Long\", \"Alt\", \"Date\", \"Month\", \"O18(1)\", \"O18(2)\", \"O18Avg\", \"H2(1)\", \"H2(2)\", \"H2avg\", \"dex\", \"Temp\", \"Prec(1)\", \"Prec(2)\", \"Prec(3)\"]\n",
    "# Combine the CNIP data into one dataframe\n",
    "cnip = pd.concat(cnipDict.values(), ignore_index=True)\n",
    "cnip.columns = columns\n",
    "\n",
    "# Print the CNIP dataframe\n",
    "cnip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove unnecessary columns and clean the data\n",
    "\n",
    "Now that it is in one dataframe I will be cleaning the data by removing any rows that have missing values, and converting the date into a date time object. This will also require that I make sure rows filled with empty strings are converted to NaN values. I will also remove columns that are not needed for the analysis, which are as follows:\n",
    "* Month\n",
    "* O18(1)\n",
    "* O18(2)\n",
    "* H2(1)\n",
    "* H2(2)\n",
    "* Temp\n",
    "* Precip(1-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Station</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Date</th>\n",
       "      <th>O18Avg</th>\n",
       "      <th>H2avg</th>\n",
       "      <th>dex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-02-02</td>\n",
       "      <td>-10.520</td>\n",
       "      <td>-50.700</td>\n",
       "      <td>33.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-03-02</td>\n",
       "      <td>-8.605</td>\n",
       "      <td>-48.040</td>\n",
       "      <td>20.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-04-02</td>\n",
       "      <td>-10.880</td>\n",
       "      <td>-27.485</td>\n",
       "      <td>59.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-05-02</td>\n",
       "      <td>-7.550</td>\n",
       "      <td>-50.455</td>\n",
       "      <td>9.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BAB</td>\n",
       "      <td>47.98</td>\n",
       "      <td>55.82</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1997-06-02</td>\n",
       "      <td>-5.835</td>\n",
       "      <td>-76.480</td>\n",
       "      <td>-29.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>3265</td>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-03-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>3266</td>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-04-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>3267</td>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-05-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>3268</td>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-06-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>3269</td>\n",
       "      <td>HAL</td>\n",
       "      <td>68.47</td>\n",
       "      <td>81.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2007-07-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3249 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index Station    Lat   Long    Alt       Date  O18Avg   H2avg     dex\n",
       "0         0     BAB  47.98  55.82  190.0 1997-02-02 -10.520 -50.700  33.460\n",
       "1         1     BAB  47.98  55.82  190.0 1997-03-02  -8.605 -48.040  20.800\n",
       "2         2     BAB  47.98  55.82  190.0 1997-04-02 -10.880 -27.485  59.555\n",
       "3         3     BAB  47.98  55.82  190.0 1997-05-02  -7.550 -50.455   9.945\n",
       "4         4     BAB  47.98  55.82  190.0 1997-06-02  -5.835 -76.480 -29.800\n",
       "...     ...     ...    ...    ...    ...        ...     ...     ...     ...\n",
       "3244   3265     HAL  68.47  81.15    8.0 2007-03-02     NaN     NaN     NaN\n",
       "3245   3266     HAL  68.47  81.15    8.0 2007-04-02     NaN     NaN     NaN\n",
       "3246   3267     HAL  68.47  81.15    8.0 2007-05-02     NaN     NaN     NaN\n",
       "3247   3268     HAL  68.47  81.15    8.0 2007-06-02     NaN     NaN     NaN\n",
       "3248   3269     HAL  68.47  81.15    8.0 2007-07-02     NaN     NaN     NaN\n",
       "\n",
       "[3249 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the date column to datetime format\n",
    "cnip[\"Date\"] = pd.to_datetime(cnip[\"Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Filling in empty strings with NaN\n",
    "cnip = cnip.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Removing unnecessary columns\n",
    "cnip = cnip.drop([\"Month\", \"O18(1)\", \"O18(2)\", \"H2(1)\", \"H2(2)\", \"Prec(1)\", \"Prec(2)\", \"Prec(3)\", \"Temp\"], axis=1)\n",
    "\n",
    "# Removing rows in the Date column with NaT values\n",
    "cnip = cnip[cnip.Date.isnull() == False].reset_index()\n",
    "cnip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract the needed information from the HydroGFD data\n",
    "There is 14 netcdf files.... But yeah we will extract that information and then combine it into one dataframe. This will require both precipitation flux and Mean temperature 2m. Downloaded [here](https://cds.climate.copernicus.eu/cdsapp#!/dataset/sis-ecv-cmip5-bias-corrected?tab=form) with these parameters:\n",
    "* Variable: Mean temperature 2m / precipitation flux\n",
    "* Model: GFDL-CM3 (NOAA, USA)\n",
    "* Experiment: RCP 4.5\n",
    "* Period: 1960 - 2010\n",
    "\n",
    "This section will be broken up into subparts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract the precipitation flux data\n",
    "This will be done by extracting the data from the netcdf files and then combining them into one dataframe. The columns that will be extracted are as follows:\n",
    "* Lat\n",
    "* Lon\n",
    "* Precipitation\n",
    "\n",
    "It is worth noting that the latitude used from the latitude are the ones that are closest to the stations latitude. The same goes for the longitude. This is done to make sure that the data is as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaxgr\\AppData\\Local\\Temp\\ipykernel_17592\\3512970601.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  precip = pd.concat([precip, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting data from 19600101-19641231\n",
      "Finished extracting data from 19650101-19691231\n",
      "Finished extracting data from 19700101-19741231\n",
      "Finished extracting data from 19750101-19791231\n",
      "Finished extracting data from 19800101-19841231\n",
      "Finished extracting data from 19850101-19891231\n",
      "Finished extracting data from 19900101-19941231\n",
      "Finished extracting data from 19950101-19991231\n",
      "Finished extracting data from 20000101-20041231\n",
      "Finished extracting data from 20050101-20051231\n",
      "Finished extracting data from 20060101-20101231\n"
     ]
    }
   ],
   "source": [
    "# All the precipitation file starts with a \"prAdjust\" and ends with a \".nc\"\n",
    "path = \"HydroGFD/prAdjust*\"\n",
    "precipFiles = glob.glob(path) #Creates a list of all the precipitation flux files relative paths\n",
    "\n",
    "# Loop through each precipitation file and extract the data which should only be separated by time\n",
    "# and store them in a single dataframe\n",
    "precip = pd.DataFrame(columns=[\"Station\",\"Lat\", \"Long\", \"Time\", \"Precipitation\"])\n",
    "\n",
    "# Loop through each file and pull out the data at each time step for every lat and lon coordinate we have in the CNIP dataset that is stored in the stationCoords dictionary\n",
    "for file in precipFiles:\n",
    "    ncid = nc.Dataset(file, \"r\")\n",
    "\n",
    "    #Pull out the time data and coordiante data\n",
    "    time = ncid.variables[\"time\"][:].filled(np.nan)\n",
    "    lat = ncid.variables[\"lat\"][:].filled(np.nan)\n",
    "    lon = ncid.variables[\"lon\"][:].filled(np.nan)\n",
    "\n",
    "    for stat, coords in stationCoords.items():\n",
    "        latIndex = (np.abs(lat - coords[0])).argmin()\n",
    "        lonIndex = (np.abs(lon - coords[1])).argmin()\n",
    "\n",
    "        # Pull out the precipitation data at each time step\n",
    "        precipData = ncid.variables[\"prAdjust\"][:, latIndex, lonIndex].filled(0) #Filling with 0 is an assumption that if there is no data, then there is no precipitation\n",
    "        \n",
    "        # Place the lat, lon, time, and precipitation data into a dataframe\n",
    "        df = pd.DataFrame({\"Station\": stat, \"Lat\": coords[0], \"Long\": coords[1], \"Time\": time, \"Precipitation\": precipData})\n",
    "        precip = pd.concat([precip, df], ignore_index=True)\n",
    "    print(\"Finished extracting data from \" + file[-20:-3])\n",
    "    ncid.close()\n",
    "\n",
    "# Convert the time data to datetime format\n",
    "precip[\"Time\"] = precip[\"Time\"].apply(lambda x: datetime(1850, 1, 1) + timedelta(days=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe by time and reset the index\n",
    "precip = precip.sort_values(by=[\"Time\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extract the mean temperature data\n",
    "This will be done almost exactly the same way as it was done for the precipitation flux data. The columns that will be extracted are as follows:\n",
    "* Lat\n",
    "* Lon\n",
    "* Temperature\n",
    "\n",
    "It is worth noting that the latitude used from the latitude are the ones that are closest to the stations latitude. The same goes for the longitude. This is done to make sure that the data is as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaxgr\\AppData\\Local\\Temp\\ipykernel_17592\\2826800507.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  temperature = pd.concat([temperature, df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting data from 19600101-19641231\n",
      "Finished extracting data from 19650101-19691231\n",
      "Finished extracting data from 19700101-19741231\n",
      "Finished extracting data from 19750101-19791231\n",
      "Finished extracting data from 19800101-19841231\n",
      "Finished extracting data from 19850101-19891231\n",
      "Finished extracting data from 19900101-19941231\n",
      "Finished extracting data from 19950101-19991231\n",
      "Finished extracting data from 20000101-20041231\n",
      "Finished extracting data from 20050101-20051231\n",
      "Finished extracting data from 20060101-20101231\n"
     ]
    }
   ],
   "source": [
    "# All the temperature files starts with a \"tasAdjust\" and ends with a \".nc\"\n",
    "path = \"HydroGFD/tasAdjust*\"\n",
    "tempFiles = glob.glob(path) #Creates a list of all the temperature flux files relative paths\n",
    "\n",
    "# Loop through each temperature file and extract the data which should only be separated by time\n",
    "# and store them in a single dataframe\n",
    "temperature = pd.DataFrame(columns=[\"Station\",\"Lat\", \"Long\", \"Time\", \"Temperature\"])\n",
    "\n",
    "# Loop through each file and pull out the data at each time step for every lat and lon coordinate we have in the CNIP dataset that is stored in the stationCoords dictionary\n",
    "for file in tempFiles:\n",
    "    ncid = nc.Dataset(file, \"r\")\n",
    "\n",
    "    #Pull out the time data and coordiante data\n",
    "    time = ncid.variables[\"time\"][:].filled(np.nan)\n",
    "    lat = ncid.variables[\"lat\"][:].filled(np.nan)\n",
    "    lon = ncid.variables[\"lon\"][:].filled(np.nan)\n",
    "\n",
    "    for stat, coords in stationCoords.items():\n",
    "        latIndex = (np.abs(lat - coords[0])).argmin()\n",
    "        lonIndex = (np.abs(lon - coords[1])).argmin()\n",
    "\n",
    "        # Pull out the temperature data at each time step\n",
    "        tempData = ncid.variables[\"tasAdjust\"][:, latIndex, lonIndex].filled(np.nan) #Filling with nan, as we can't make an assumption about the temperature\n",
    "        \n",
    "        # Place the lat, lon, time, and temperature data into a dataframe\n",
    "        df = pd.DataFrame({\"Station\": stat, \"Lat\": coords[0], \"Long\": coords[1], \"Time\": time, \"Temperature\": tempData})\n",
    "        temperature = pd.concat([temperature, df], ignore_index=True)\n",
    "    print(\"Finished extracting data from \" + file[-20:-3])\n",
    "    ncid.close()\n",
    "\n",
    "# Convert the time data to datetime format\n",
    "temperature[\"Time\"] = temperature[\"Time\"].apply(lambda x: datetime(1850, 1, 1) + timedelta(days=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the dataframe by time and reset the index\n",
    "temperature = temperature.sort_values(by=[\"Time\"]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Combine the precipitation flux and mean temperature data\n",
    "Now that we have the precipitation flux and mean temperature data we will combine them into one dataframe. This will require that we merge the two dataframes on latitude, longitude, station, and time. This will be done using the pandas merge function. The columns that will be in the final dataframe are as follows:\n",
    "* Station\n",
    "* Lat\n",
    "* Lon\n",
    "* Time\n",
    "* Precipitation\n",
    "* Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the precipitation and temperature dataframes\n",
    "hydroGFD = pd.merge(precip, temperature, on=[\"Station\", \"Lat\", \"Long\", \"Time\"], how=\"outer\")\n",
    "\n",
    "# Keep only the needed columns of the dataframe: station, lat, long, time, precipitation, temperature\n",
    "# removing the index columns\n",
    "hydroGFD = hydroGFD[[\"Station\", \"Lat\", \"Long\", \"Time\", \"Precipitation\", \"Temperature\"]]\n",
    "\n",
    "# Renaming some columns to include units\n",
    "hydroGFD = hydroGFD.rename(columns={\n",
    "    \"Precipitation\": \"Precipitation (kg/m^2/s)\", \n",
    "    \"Temperature\": \"Temperature (K)\",\n",
    "    \"Time\": \"Date\"})\n",
    "\n",
    "# Finally saving this data as a CSV file just in case\n",
    "hydroGFD.to_csv(r\"hydroGFD.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine CNIP and HydroGFD data into one dataframe\n",
    "Now that we have the CNIP data and the HydroGFD data we will combine them into one dataframe. This will require that we merge the two dataframes on latitude, longitude, station, and time. This will be done using the pandas merge function. The columns that will be in the final dataframe are as follows:\n",
    "* Station\n",
    "* Lat\n",
    "* Lon\n",
    "* Alt\n",
    "* Time / Date\n",
    "* Precipitation (kg/m^2/s)\n",
    "* Temperature (K)\n",
    "* O18avg (‰)\n",
    "* H2avg (‰)\n",
    "* Dex (‰)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot insert level_0, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17592\\3604322751.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhydroGFD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhydroGFD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Remove the rows in the CNIP dataset that are not in the HydroGFD dataset and vice versa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcnip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhydroGFD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mhydroGFD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhydroGFD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhydroGFD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Merge the CNIP and HydroGFD datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jaxgr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[0;32m   6216\u001b[0m                     level_values = algorithms.take(\n\u001b[0;32m   6217\u001b[0m                         \u001b[0mlevel_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6218\u001b[0m                     \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6220\u001b[1;33m                 new_obj.insert(\n\u001b[0m\u001b[0;32m   6221\u001b[0m                     \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6222\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6223\u001b[0m                     \u001b[0mlevel_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jaxgr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   4927\u001b[0m                 \u001b[1;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4928\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4929\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4930\u001b[0m             \u001b[1;31m# Should this be a different kind of error??\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4931\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mcannot insert \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m, already exists\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4932\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4933\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loc must be int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4934\u001b[0m         \u001b[1;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot insert level_0, already exists"
     ]
    }
   ],
   "source": [
    "# I need to make sure that CNIP and HydroGFD have the same date format so that I can merge them\n",
    "# I will also make sure that the date range is the same for both datasets\n",
    "# Convert the date column to datetime format\n",
    "cnip[\"Date\"] = pd.to_datetime(cnip[\"Date\"]).dt.date\n",
    "hydroGFD[\"Date\"] = pd.to_datetime(hydroGFD[\"Date\"]).dt.date\n",
    "\n",
    "# Remove the rows in the CNIP dataset that are not in the HydroGFD dataset and vice versa\n",
    "cnip = cnip[cnip.Date.isin(hydroGFD.Date)].reset_index()\n",
    "hydroGFD = hydroGFD[hydroGFD.Date.isin(cnip.Date)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the CNIP and HydroGFD datasets\n",
    "combined = pd.merge(cnip, hydroGFD, on=[\"Station\", \"Lat\", \"Long\", \"Date\"], how=\"outer\")\n",
    "\n",
    "# Sort the dataframe by date and reset the index\n",
    "combined = combined.sort_values(by=[\"Date\"]).reset_index()\n",
    "\n",
    "# Remove the index column\n",
    "combined = combined.drop([\"index\", \"index_x\", \"index_y\", \"level_0\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove rows with missing values and save the dataframe as a csv file\n",
    "Now that we have the combined dataframe we will remove any rows that have missing values. This will be done using the pandas dropna function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows with NaN values\n",
    "dataset = combined.dropna().reset_index()\n",
    "\n",
    "# Remove the index column\n",
    "dataset = dataset.drop([\"index\"], axis=1)\n",
    "\n",
    "# Make sure that the numeric columns are in the float64 format\n",
    "dataset[\"O18Avg\"] = dataset[\"O18Avg\"].astype(\"float64\")\n",
    "dataset[\"H2avg\"] = dataset[\"H2avg\"].astype(\"float64\")\n",
    "dataset[\"dex\"] = dataset[\"dex\"].astype(\"float64\")\n",
    "dataset[\"Precipitation (kg/m^2/s)\"] = dataset[\"Precipitation (kg/m^2/s)\"].astype(\"float64\")\n",
    "dataset[\"Temperature (K)\"] = dataset[\"Temperature (K)\"].astype(\"float64\")\n",
    "dataset[\"Alt\"] = dataset[\"Alt\"].astype(\"float64\")\n",
    "dataset[\"Lat\"] = dataset[\"Lat\"].astype(\"float64\")\n",
    "dataset[\"Long\"] = dataset[\"Long\"].astype(\"float64\")\n",
    "\n",
    "# Finally saving this data as a CSV file\n",
    "dataset.to_csv(r\"Isoscape_Data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
