{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isoNet Sample Points\n",
    "====================\n",
    "I will be using this notebook to generate the sample points for the isoNet dataset. I need the HydroGFD files as well, which will not be visible in this folder if you are viewing this on Github or downloaded from there. The instructions on dowloading them are in the data section of the code. It will be the exact same dataset used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 21:06:52.471780: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 21:06:52.514470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-21 21:06:52.514502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-21 21:06:52.515699: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-21 21:06:52.525033: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-21 21:06:52.525969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 21:06:53.524396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import netCDF4 as nc\n",
    "import glob\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in sample data from csv\n",
    "samplePoints = pd.read_csv('SamplePoints_Alt.csv')\n",
    "\n",
    "# Change Alt (m) to just Alt\n",
    "samplePoints = samplePoints.rename(columns={'Alt (m)': 'Alt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in dates for the data\n",
    "I need data from 1988 to 2010. Every lat lon coordinate must have data for every day in that range. I will use the HydroGFD data to get the dates for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45416/555233098.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  isoNet_Sample = pd.concat([isoNet_Sample, temp], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0%\r"
     ]
    }
   ],
   "source": [
    "# I need to create a new dataframe, I will do this by cycling through the samplePoints dataframe and creating a new dataframe with the same columns\n",
    "# However, this time at each row within the samplePoints dataframe, I will copy that information with the date range of 1988-2010.\n",
    "# This will mean that each coordinate will have monthly data for 22 years.\n",
    "\n",
    "# Create a new dataframe\n",
    "isoNet_Sample = pd.DataFrame(columns=['Lat', 'Lon', 'Alt', 'Year', 'Month'])\n",
    "\n",
    "# Create a list of years\n",
    "years = list(range(1988, 2011))\n",
    "\n",
    "# Create a list of months\n",
    "months = list(range(1, 13))\n",
    "\n",
    "n = len(years) * len(months)\n",
    "\n",
    "# Cycle through the samplePoints dataframe\n",
    "for index, row in samplePoints.iterrows():\n",
    "    temp = pd.DataFrame([row for _ in range(n)])\n",
    "\n",
    "    # Add the years and days to the dataframe\n",
    "    temp['Year'] = np.repeat(years, len(months))\n",
    "    temp['Month'] = np.tile(months, len(years))\n",
    "\n",
    "    # Append the temp dataframe to the isoNet_Sample dataframe\n",
    "    isoNet_Sample = pd.concat([isoNet_Sample, temp], ignore_index=True)\n",
    "    \n",
    "    percent = (index + 1) / len(samplePoints) * 100\n",
    "    print('Progress: ' + str(round(percent, 2)) + '%', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the HydroGFD data separately using the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of coords\n",
    "stationCoords = pd.read_csv(\"latlon_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45416/1179160603.py:29: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  precip = pd.concat([precip, df], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m lonIndex \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mabs(lon \u001b[38;5;241m-\u001b[39m coords[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39margmin()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Pull out the precipitation data at each time step\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m precipData \u001b[38;5;241m=\u001b[39m \u001b[43mncid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprAdjust\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlonIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfilled(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#Filling with 0 is an assumption that if there is no data, then there is no precipitation\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Place the lat, lon, time, and precipitation data into a dataframe\u001b[39;00m\n\u001b[1;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLat\u001b[39m\u001b[38;5;124m\"\u001b[39m: coords[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLon\u001b[39m\u001b[38;5;124m\"\u001b[39m: coords[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecipitation\u001b[39m\u001b[38;5;124m\"\u001b[39m: precipData})\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5011\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable.__getitem__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5081\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable._toma\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:5378\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Variable._check_safecast\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Projects/ML_Thesis_Code/.venv/lib/python3.11/site-packages/netCDF4/utils.py:15\u001b[0m, in \u001b[0;36m_safecast\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# no bytes type in python < 2.6\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safecast\u001b[39m(a,b):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# check to see if array a can be safely cast\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# to array b.  A little less picky than numpy.can_cast.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         is_safe \u001b[38;5;241m=\u001b[39m ((a \u001b[38;5;241m==\u001b[39m b) \u001b[38;5;241m|\u001b[39m (np\u001b[38;5;241m.\u001b[39misnan(a) \u001b[38;5;241m&\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(b)))\u001b[38;5;241m.\u001b[39mall()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Starting with precip files \n",
    "path = \"HydroGFD/prAdjust*\"\n",
    "precipFiles = glob.glob(path)\n",
    "precipFiles.sort()\n",
    "precipFiles\n",
    "\n",
    "# Create an empty dataframe to store the data from the netCDF files, with the columns: lat, lon, Time, and Precip\n",
    "precip = pd.DataFrame(columns=['Lat', 'Lon', 'Time', 'Precip'])\n",
    "\n",
    "# Cycle through the precipFiles\n",
    "# Loop through each file and pull out the data at each time step for every lat and lon coordinate we have in the CNIP dataset that is stored in the stationCoords dictionary\n",
    "for file in precipFiles:\n",
    "    ncid = nc.Dataset(file, \"r\")\n",
    "\n",
    "    #Pull out the time data and coordiante data\n",
    "    time = ncid.variables[\"time\"][:].filled(np.nan)\n",
    "    lat = ncid.variables[\"lat\"][:].filled(np.nan)\n",
    "    lon = ncid.variables[\"lon\"][:].filled(np.nan)\n",
    "\n",
    "    for coords in stationCoords.itertuples(index=False):\n",
    "        latIndex = (np.abs(lat - coords[0])).argmin()\n",
    "        lonIndex = (np.abs(lon - coords[1])).argmin()\n",
    "\n",
    "        # Pull out the precipitation data at each time step\n",
    "        precipData = ncid.variables[\"prAdjust\"][:, latIndex, lonIndex].filled(0) #Filling with 0 is an assumption that if there is no data, then there is no precipitation\n",
    "        \n",
    "        # Place the lat, lon, time, and precipitation data into a dataframe\n",
    "        df = pd.DataFrame({\"Lat\": coords[0], \"Lon\": coords[1], \"Time\": time, \"Precipitation\": precipData})\n",
    "        precip = pd.concat([precip, df], ignore_index=True)\n",
    "    print(\"Finished extracting data from \" + file[-20:-3])\n",
    "    ncid.close()\n",
    "\n",
    "# Convert the time data to datetime format\n",
    "precip[\"Time\"] = precip[\"Time\"].apply(lambda x: datetime(1850, 1, 1) + timedelta(days=x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
